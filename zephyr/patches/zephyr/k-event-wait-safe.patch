From 3321e4e10571dac039a8b7f23f0b9697f0b972cb Mon Sep 17 00:00:00 2001
From: Chris Friedt <cfriedt@tenstorrent.com>
Date: Fri, 9 May 2025 09:24:28 -0400
Subject: [PATCH 1/4] kernel: events: introduce internal K_EVENT_WAIT_REMOVE
 flag

Introduce the internal K_EVENT_WAIT_REMOVE flag, which instructs
k_event_wait_internal() to remove matched events (all or enay)
from the set of events stored in the event object.

Signed-off-by: Chris Friedt <cfriedt@tenstorrent.com>
---
 kernel/events.c | 60 +++++++++++++++++++++++++++++--------------------
 1 file changed, 36 insertions(+), 24 deletions(-)

diff --git a/kernel/events.c b/kernel/events.c
index 038697d931f9..531702238054 100644
--- a/kernel/events.c
+++ b/kernel/events.c
@@ -36,9 +36,10 @@
 
 #define K_EVENT_WAIT_ANY      0x00   /* Wait for any events */
 #define K_EVENT_WAIT_ALL      0x01   /* Wait for all events */
-#define K_EVENT_WAIT_MASK     0x01
-
 #define K_EVENT_WAIT_RESET    0x02   /* Reset events prior to waiting */
+#define K_EVENT_WAIT_REMOVE   0x04   /* Remove matched events from the event object */
+
+#define K_EVENT_WAIT_MASK K_EVENT_WAIT_ALL
 
 struct event_walk_data {
 	struct k_thread  *head;
@@ -77,7 +78,7 @@ void z_vrfy_k_event_init(struct k_event *event)
 #endif /* CONFIG_USERSPACE */
 
 /**
- * @brief determine if desired set of events been satisfied
+ * @brief determine the set of events that have been been satisfied
  *
  * This routine determines if the current set of events satisfies the desired
  * set of events. If @a wait_condition is K_EVENT_WAIT_ALL, then at least
@@ -85,30 +86,39 @@ void z_vrfy_k_event_init(struct k_event *event)
  * wait_condition is not K_EVENT_WAIT_ALL, it is assumed to be K_EVENT_WAIT_ANY.
  * In the K_EVENT_WAIT_ANY case, the request is satisfied when any of the
  * current set of events are present in the desired set of events.
+ *
+ * @return 0 if the wait condition is not satisfied, otherwise the set of
+ * events that satisfy the wait condition.
  */
-static bool are_wait_conditions_met(uint32_t desired, uint32_t current,
-				    unsigned int wait_condition)
+static uint32_t are_wait_conditions_met(uint32_t desired, uint32_t current,
+					unsigned int wait_condition)
 {
-	uint32_t  match = current & desired;
+	uint32_t match = current & desired;
 
-	if (wait_condition == K_EVENT_WAIT_ALL) {
-		return match == desired;
+	if ((wait_condition == K_EVENT_WAIT_ALL) && (match != desired)) {
+		return 0;
 	}
 
 	/* wait_condition assumed to be K_EVENT_WAIT_ANY */
 
-	return match != 0;
+	return match;
 }
 
 static int event_walk_op(struct k_thread *thread, void *data)
 {
+	uint32_t match;
 	unsigned int      wait_condition;
 	struct event_walk_data *event_data = data;
 
 	wait_condition = thread->event_options & K_EVENT_WAIT_MASK;
 
-	if (are_wait_conditions_met(thread->events, event_data->events,
-				    wait_condition)) {
+	match = are_wait_conditions_met(thread->events, event_data->events, wait_condition);
+	if (match != 0) {
+
+		/*
+		 * Overwrite the threads mask of events with the exact match.
+		 */
+		thread->events = match;
 
 		/*
 		 * Events create a list of threads to wake up. We do
@@ -166,7 +176,6 @@ static uint32_t k_event_post_internal(struct k_event *event, uint32_t events,
 		struct k_thread *next;
 		do {
 			arch_thread_return_value_set(thread, 0);
-			thread->events = events;
 			next = thread->next_event_link;
 			z_sched_wake_thread(thread, false);
 			thread = next;
@@ -268,16 +277,17 @@ static uint32_t k_event_wait_internal(struct k_event *event, uint32_t events,
 
 	/* Test if the wait conditions have already been met. */
 
-	if (are_wait_conditions_met(events, event->events, wait_condition)) {
-		rv = event->events;
-
-		k_spin_unlock(&event->lock, key);
-		goto out;
+	rv = are_wait_conditions_met(events, event->events, wait_condition);
+	if ((rv != 0) && ((options & K_EVENT_WAIT_REMOVE) != 0)) {
+		/* Remove events that have been received */
+		event->events &= ~rv;
 	}
 
-	/* Match conditions have not been met. */
-
-	if (K_TIMEOUT_EQ(timeout, K_NO_WAIT)) {
+	if (K_TIMEOUT_EQ(timeout, K_NO_WAIT) || (rv != 0)) {
+		/*
+		 * Match conditions have either already been met, or the caller specified
+		 * K_NO_WAIT, and match conditions have not been met.
+		 */
 		k_spin_unlock(&event->lock, key);
 		goto out;
 	}
@@ -294,15 +304,17 @@ static uint32_t k_event_wait_internal(struct k_event *event, uint32_t events,
 					   options, timeout);
 
 	if (z_pend_curr(&event->lock, key, &event->wait_q, timeout) == 0) {
-		/* Retrieve the set of events that woke the thread */
 		rv = thread->events;
+		if (options & K_EVENT_WAIT_REMOVE) {
+			/* Remove events that have been received */
+			event->events &= ~rv;
+		}
 	}
 
 out:
-	SYS_PORT_TRACING_OBJ_FUNC_EXIT(k_event, wait, event,
-				       events, rv & events);
+	SYS_PORT_TRACING_OBJ_FUNC_EXIT(k_event, wait, event, events, rv);
 
-	return rv & events;
+	return rv;
 }
 
 /**

From e4678b45cc285d0eeec09cfebec769127c831db3 Mon Sep 17 00:00:00 2001
From: Chris Friedt <cfriedt@tenstorrent.com>
Date: Fri, 9 May 2025 09:29:02 -0400
Subject: [PATCH 2/4] kernel: events: introduce safe variants of k_event_wait()

Previously, the k_event_wait() and k_event_wait_all() APIs had a
shortcoming, in that they received events could never be cleared in a
safe way.

The only way to clear individual events was with k_event_clear(),
but that is inherently racey, and could result in lost event
information even when using a single thread and an interrupt.

The alternative, setting the "reset" parameter of k_event_wait() or
k_event_wait_all(), was also unsafe since it would clear all events,
leading to event information loss.

Without a safe way to clear events, the application would naturally
experience "phantom" events. I.e. events that were not cleared from
previous k_event_wait() operations and actually never occurred.

Note: since the events are stored as a bitmask and are not queued, it
is not possible to know if events have been issues multiple times
prior to being serviced.

Event information loss, as well as phantom events, could be considered
somewhat unsafe, regardless of whether events were edge triggered
or level triggered.

This change introduces two companion syscalls that allow us to wait for
events in a safe way; k_event_wait_safe() and k_event_wait_all_safe().

These two new system calls use the internal K_EVENT_WAIT_REMOVE flag
to indicate to k_event_wait_internal(), that it should atomically
remove the matching, received events from the set of events contained
in the event object.

With that, callers will not receive phantom events. Each time an
event is ready, callers can be certain that it was an actual event.

Similarly, there is no explicit need to reset or clear events manually,
since the received events are automatically cleared by
k_event_wait_safe() and k_event_wait_all_safe().

Signed-off-by: Chris Friedt <cfriedt@tenstorrent.com>
---
 include/zephyr/kernel.h | 39 +++++++++++++++++++++++++++++++++++++++
 kernel/events.c         | 30 ++++++++++++++++++++++++++++++
 2 files changed, 69 insertions(+)

diff --git a/include/zephyr/kernel.h b/include/zephyr/kernel.h
index 410d7bf90c90..650e752c9183 100644
--- a/include/zephyr/kernel.h
+++ b/include/zephyr/kernel.h
@@ -2507,6 +2507,45 @@ __syscall uint32_t k_event_wait(struct k_event *event, uint32_t events,
 __syscall uint32_t k_event_wait_all(struct k_event *event, uint32_t events,
 				    bool reset, k_timeout_t timeout);
 
+/**
+ * @brief Wait for any of the specified events (safe version)
+ *
+ * This call is nearly identical to @ref k_event_wait with the main difference
+ * being that the safe version atomically clears receieved events from the
+ * event object. This mitigates the need for calling @ref k_event_clear, or
+ * passing a "reset" argument, since doing so may result in lost event
+ * information.
+ *
+ * @param event Address of the event object
+ * @param events Set of desired events on which to wait
+ * @param timeout Waiting period for the desired set of events or one of the
+ *                special values K_NO_WAIT and K_FOREVER.
+ *
+ * @retval set of matching events upon success
+ * @retval 0 if no matching event was received within the specified time
+ */
+__syscall uint32_t k_event_wait_safe(struct k_event *event, uint32_t events, k_timeout_t timeout);
+
+/**
+ * @brief Wait for all of the specified events (safe version)
+ *
+ * This call is nearly identical to @ref k_event_wait_all with the main
+ * difference being that the safe version atomically clears receieved events
+ * from the event object. This mitigates the need for calling
+ * @ref k_event_clear, or passing a "reset" argument, since doing so may
+ * result in lost event information.
+ *
+ * @param event Address of the event object
+ * @param events Set of desired events on which to wait
+ * @param timeout Waiting period for the desired set of events or one of the
+ *                special values K_NO_WAIT and K_FOREVER.
+ *
+ * @retval set of matching events upon success
+ * @retval 0 if all matching events were not received within the specified time
+ */
+__syscall uint32_t k_event_wait_all_safe(struct k_event *event, uint32_t events,
+					 k_timeout_t timeout);
+
 /**
  * @brief Test the events currently tracked in the event object
  *
diff --git a/kernel/events.c b/kernel/events.c
index 531702238054..f7b0d8c8ba14 100644
--- a/kernel/events.c
+++ b/kernel/events.c
@@ -359,6 +359,36 @@ uint32_t z_vrfy_k_event_wait_all(struct k_event *event, uint32_t events,
 #include <zephyr/syscalls/k_event_wait_all_mrsh.c>
 #endif /* CONFIG_USERSPACE */
 
+uint32_t z_impl_k_event_wait_safe(struct k_event *event, uint32_t events, k_timeout_t timeout)
+{
+	return k_event_wait_internal(event, events, K_EVENT_WAIT_REMOVE, timeout);
+}
+
+#ifdef CONFIG_USERSPACE
+uint32_t z_vrfy_k_event_wait_safe(struct k_event *event, uint32_t events,
+				  k_timeout_t timeout)
+{
+	K_OOPS(K_SYSCALL_OBJ(event, K_OBJ_EVENT));
+	return z_impl_k_event_wait_safe(event, events, timeout);
+}
+#include <zephyr/syscalls/k_event_wait_safe_mrsh.c>
+#endif /* CONFIG_USERSPACE */
+
+uint32_t z_impl_k_event_wait_all_safe(struct k_event *event, uint32_t events, k_timeout_t timeout)
+{
+	return k_event_wait_internal(event, events, K_EVENT_WAIT_ALL | K_EVENT_WAIT_REMOVE,
+				     timeout);
+}
+
+#ifdef CONFIG_USERSPACE
+uint32_t z_vrfy_k_event_wait_all_safe(struct k_event *event, uint32_t events, k_timeout_t timeout)
+{
+	K_OOPS(K_SYSCALL_OBJ(event, K_OBJ_EVENT));
+	return z_impl_k_event_wait_all_safe(event, events, timeout);
+}
+#include <zephyr/syscalls/k_event_wait_all_safe_mrsh.c>
+#endif /* CONFIG_USERSPACE */
+
 #ifdef CONFIG_OBJ_CORE_EVENT
 static int init_event_obj_core_list(void)
 {

